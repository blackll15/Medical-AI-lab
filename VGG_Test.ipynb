{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG_Test.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyO8OZPkGR2RgRI2lVhZBgk5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Spxy5AWAOrh-","colab_type":"code","outputId":"dc14fc3d-1651-4b61-9824-4f2df16f32ba","executionInfo":{"status":"ok","timestamp":1589691619756,"user_tz":-540,"elapsed":696,"user":{"displayName":"김성우","photoUrl":"","userId":"02237163950493872911"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","directory_path = '/content/drive/My Drive/lab/'"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VGmnjXZtOxRL","colab_type":"code","colab":{}},"source":["# Auther: Seongwoo Kim\n","#한 이미지 내의 연속된 숫자를 자르는 프로그램\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import cv2\n","import os\n","from torchvision import datasets, transforms, utils\n","from torch.utils.data import Dataset\n","from torchvision.transforms import ToTensor\n","import torch.utils.model_zoo as model_zoo\n","from PIL import Image\n","\n","\n","USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","class VGG(nn.Module):\n","    def __init__(self, features, num_classes=1000, init_weights=True):\n","        super(VGG, self).__init__()\n","        \n","        self.features = features #convolution\n","        \n","        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","        \n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes),\n","        )#FC layer\n","        \n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.features(x) #Convolution \n","        x = self.avgpool(x) # avgpool\n","        x = x.view(x.size(0), -1) #\n","        x = self.classifier(x) #FC layer\n","        return F.log_softmax(x, dim=1)\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","def make_layers(cfg, batch_norm=False):\n","    layers = []\n","    in_channels = 1\n","    \n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","                     \n","    return nn.Sequential(*layers)\n","cfg = {\n","    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], #8 + 3 =11 == vgg11\n","    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], # 10 + 3 = vgg 13\n","    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], #13 + 3 = vgg 16\n","    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'] # 16 +3 =vgg 19\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aeqb6u-JOz34","colab_type":"code","outputId":"67d75506-544a-4c44-9fc7-8005478beb2d","executionInfo":{"status":"ok","timestamp":1589691639824,"user_tz":-540,"elapsed":8531,"user":{"displayName":"김성우","photoUrl":"","userId":"02237163950493872911"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["count_model = VGG(make_layers(cfg['A']), num_classes=4, init_weights=True).to(DEVICE)\n","count_model.load_state_dict(torch.load(directory_path + 'VGG_count_model.pt'))\n","center_model = VGG(make_layers(cfg['B']), num_classes=20, init_weights=True).to(DEVICE)\n","center_model.load_state_dict(torch.load(directory_path + 'VGG_center_model.pt'))\n","rec_model = VGG(make_layers(cfg['A']), num_classes=10, init_weights=True).to(DEVICE)\n","rec_model.load_state_dict(torch.load(directory_path + 'VGG_ver2_model.pt'))"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"rVKZOYAnO5tx","colab_type":"code","outputId":"c64b377f-d509-4d7b-8d03-70480df3acd1","executionInfo":{"status":"ok","timestamp":1589692253055,"user_tz":-540,"elapsed":32553,"user":{"displayName":"김성우","photoUrl":"","userId":"02237163950493872911"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["def process(pred, num):\n","  _, length = pred.shape\n","  peak_list = list()\n","  for n in range(num):\n","    targetidx = pred.max(dim=1)[1].item()\n","    peak_list.append(targetidx)\n","    for j in range(-1, 2):\n","      if(0 <= targetidx+j  and targetidx+j < length):\n","        pred[0,targetidx+j].sub_(2)\n","  peak_list.sort()\n","  return peak_list\n","\n","def showimg(tensor):\n","  npimg   = utils.make_grid(tensor, padding=0)\n","  npimg = npimg.numpy()\n","  plt.figure(figsize=(20, 14))\n","  plt.imshow(np.transpose(npimg, (1,2,0)))\n","  plt.show()\n","  \n","def find_contour(img):\n","  Img = (img * 255).type(dtype=torch.uint8)\n","  Img = Img.cpu().numpy().transpose(1, 2, 0)\n","  _,W, H = img.shape\n","  Img_size = W * H\n","  contours, hierachy = cv2.findContours(Img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","  group = list()\n","  for n in range(len(contours)):\n","      area = cv2.contourArea(contours[n])\n","      if(area > Img_size * 0.0008  and hierachy[0,n,3] == -1):\n","          group.append((area, n))\n","  group.sort()\n","  if(len(group)== 0):\n","    print('error')\n","    return torch.zeros(1,28,28)\n","  \n","  for n in range(len(group)):\n","    x, y, w, h = cv2.boundingRect(contours[group[n][1]])\n","    if(n < len(group) - 1):\n","      img[:,y:y+h,x:x+w] = 0\n","  return img[:,y:y+h,x:x+w]\n","\n","def resizing(t, y, x):\n","  _,_,height,width = t.shape\n","  res = torch.zeros(1,1,y,x).to(DEVICE)\n","  if(height/width > y/x):\n","    t = F.interpolate(t, size=(y, round(width * y / height)), mode='area')\n","    _,_,height,width = t.shape\n","    res[:,:,:,0:width].add_(t)\n","  else:\n","    res = F.interpolate(t, size=(y, x), mode='area').to(DEVICE)\n","  return res\n","\n","def tocenter(t,x, x_rad):\n","  _,height,width = t.shape\n","  if(x + x_rad < width):\n","    t = t[:,:,:x + x_rad]\n","  else:\n","    t = torch.cat((t,torch.zeros(1,height,x + x_rad - width).to(DEVICE)), dim=2)\n","  if(x > x_rad):\n","    t = t[:,:,x - x_rad:]\n","  else:\n","    t = torch.cat((torch.zeros(1,height,x_rad - x).to(DEVICE),t), dim=2)\n","  return t\n","\n","adaptive = False\n","correct_total = 0\n","total = 0\n","lendiff = 0\n","sample_list = [24]\n","for sample_idx in range(len(sample_list)*100):\n","  test_num = 100*sample_list[sample_idx//100] + sample_idx%100\n","  total += 1\n","  img_path=directory_path + \"seq_image/seq\"+str(test_num)+\".jpg\"\n","  Img = cv2.imread(img_path)\n","  Img = cv2.cvtColor(Img, cv2.COLOR_BGR2GRAY)\n","  Img = cv2.GaussianBlur(Img,(5,5),0)\n","\n","  if(adaptive):\n","    #adaptive thresholding 사용시\n","    Img = cv2.adaptiveThreshold(Img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 5)\n","  else:\n","    #Img = cv2.adaptiveThreshold(Img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 5)\n","    ret,Img = cv2.threshold(Img,0,255,cv2.THRESH_OTSU)\n","  if(Img.mean() > 127):\n","      ret,Img = cv2.threshold(Img,0,255,cv2.THRESH_BINARY_INV)\n","  W, H = Img.shape\n","  Img_size = W * H\n","\n","  contours, hierachy = cv2.findContours(Img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","\n","  Img = ToTensor()(Img)\n","\n","  di = dict() #key: 특정 크기 이상의 컨투어의 인덱스 value: 탐색이 진행되었는지\n","  for n in range(len(contours)):\n","      area = cv2.contourArea(contours[n])\n","      if(area > Img_size * 0.0008 and area < Img_size * 0.8):\n","          #print(area, n)\n","          if(hierachy[0,n,3] == -1):di[n] = False\n","  group = list()\n","\n","  def findpeer(ele, L, D):\n","      if(ele not in D or D[ele] == True):\n","        return\n","      D[ele] = True\n","      L.append(ele)\n","      front = hierachy[0, ele, 0].item()\n","      back = hierachy[0, ele, 1].item()\n","      if(front != -1): findpeer(front, L, D)\n","      if(back != -1): findpeer(back, L, D)\n","    \n","  for key in di:\n","      if(di[key] == True):\n","          continue\n","      L = list()\n","      group.append(L)\n","      findpeer(key, L, di)\n","\n","\n","  group.sort(key=len)\n","  Imgs = list()\n","\n","  if(len(group)== 0):\n","    raise Exception('글자를 인식하지 못했습니다')\n","\n","  for idx in group[len(group)-1]:\n","      x, y, w, h = cv2.boundingRect(contours[idx])\n","      empty_image = torch.zeros(1,h,w)\n","      empty_image.add_(Img[:, y:y+h, x:x+w])\n","      Imgs.append((x,empty_image,w/h))\n","  Imgs.sort()\n","\n","  res = ''\n","  for n in range(len(Imgs)):          #각 숫자의 이미지를 담은 리스트에서 이미지 추출\n","      Img = Imgs[n][1]\n","      Img = find_contour(Img).unsqueeze(0)\n","      rImg = resizing(Img.to(DEVICE), 224,336) #224 x 224 크기로 이미지 사이즈 조절\n","      digit_count = count_model(rImg).max(1, keepdim=True)[1].item() + 1   #이미지 내의 숫자가 몇자리숫자인지 반환\n","      center_point = F.softmax(center_model(F.interpolate(Img.to(DEVICE), size=(224,336), mode='area')))                             #slice모델에서 결과 받은 결과를 softmax\n","      peak_list = process(center_point, digit_count)                         #3자리 숫자일 경우 3개의 peak를 골라줌\n","      if(digit_count == 1): peak_list = [10]\n","      sliced_img = list()                                                 #자른 이미지가 들어갈 리스트\n","      Img = F.interpolate(Img, size=(224,round(224*Imgs[n][2])), mode='area').to(DEVICE)\n","      for ele in peak_list:\n","          centre = round(ele * 224*Imgs[n][2] / 19)\n","          sliced_img.append(tocenter(Img.squeeze(0),centre,168).unsqueeze(0))           #peaklist를 토대로 연속된 숫자 자름\n","      for ele in sliced_img:                                              \n","          img_transform = transforms.Compose([\n","                         transforms.Normalize((0.1307,),(0.3881,))\n","                         #MNIST 데이터셋의 평균과 표준편차                \n","          ])\n","          Img = img_transform(ele.squeeze(0)).unsqueeze(0).to(DEVICE)\n","          rec_model.eval()\n","          output = rec_model(Img)\n","    #CNN모델에서 숫자를 인식한 뒤 softmax된 output 받음\n","          pred = output.max(1, keepdim=True)[1].item()\n","    #가장 확률이 높은 인덱스를 예측값으로 받음\n","          res += str(pred)\n","  if(int(res) == test_num):correct_total += 1\n","  if(len(res) != len(str(test_num))):lendiff += 1\n","  #print(res, test_num)\n","print(lendiff)\n","print('Correct: {} Total: {}, Accuracy: {:.2f}%'.format(\n","          correct_total, total, correct_total / total * 100))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:138: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["6\n","Correct: 74 Total: 100, Accuracy: 74.00%\n"],"name":"stdout"}]}]}